{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./../src/\")\n",
    "from utilities import REPO_PATH, DATA_PATH, RESPONSE_COL_NAME, get_feature_corr_with_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "test = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "val = pd.read_csv(f\"{DATA_PATH}/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop([RESPONSE_COL_NAME], axis=1), train[RESPONSE_COL_NAME]\n",
    "y_test, X_test = test[RESPONSE_COL_NAME], test.drop([RESPONSE_COL_NAME],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Counter({True: 17000, False: 424})\n"
     ]
    }
   ],
   "source": [
    "beforeCounter = Counter(y_train)\n",
    "print(\"Before:\", beforeCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ratios = np.arange(0.1,1.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: Counter({True: 17000, False: 1700})\n",
      "Accuracy: 0.6607142857142857\n",
      " \n",
      "After: Counter({True: 17000, False: 3400})\n",
      "Accuracy: 0.6678571428571428\n",
      " \n",
      "After: Counter({True: 17000, False: 5100})\n",
      "Accuracy: 0.6714285714285714\n",
      " \n",
      "After: Counter({True: 17000, False: 6800})\n",
      "Accuracy: 0.6785714285714286\n",
      " \n",
      "After: Counter({True: 17000, False: 8500})\n",
      "Accuracy: 0.675\n",
      " \n",
      "After: Counter({True: 17000, False: 10200})\n",
      "Accuracy: 0.6714285714285714\n",
      " \n",
      "After: Counter({True: 17000, False: 11900})\n",
      "Accuracy: 0.6857142857142857\n",
      " \n",
      "After: Counter({True: 17000, False: 13600})\n",
      "Accuracy: 0.6821428571428572\n",
      " \n",
      "After: Counter({True: 17000, False: 15300})\n",
      "Accuracy: 0.6857142857142857\n",
      " \n",
      "After: Counter({False: 17000, True: 17000})\n",
      "Accuracy: 0.6821428571428572\n",
      " \n"
     ]
    }
   ],
   "source": [
    "accuracy_scores_by_ratio = []\n",
    "for ratio in sample_ratios:\n",
    "    smt = SMOTE(sampling_strategy=ratio)\n",
    "    X_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n",
    "    afterCounter = Counter(y_train_sm)\n",
    "    print(\"After:\", afterCounter)\n",
    "\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(X_train_sm, y_train_sm)\n",
    "    y_preds = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_preds, y_test)\n",
    "    accuracy_scores_by_ratio.append(accuracy)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sample_ratios, accuracy_scores_by_ratio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a GAN to generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = pd.read_csv(f'{DATA_PATH}/positive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX,py = positive_data.drop([RESPONSE_COL_NAME],axis=1), positive_data[RESPONSE_COL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X.values.astype(np.float32)\n",
    "        self.labels = y.values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = {\n",
    "            'input': torch.tensor(self.data[idx]),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Block Function\n",
    "def FC_Layer_blockGen(input_dim, output_dim):\n",
    "    single_block = nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return single_block\n",
    "\n",
    "# Discriminattor Block Function   \n",
    "def FC_Layer_BlockDisc(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training parameters\n",
    "batch_size = 256\n",
    "num_epochs = 500\n",
    "lr = 0.0002\n",
    "num_features = 62\n",
    "latent_dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data dimensions\n",
    "noise_dim = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL INITIALIZATION\n",
    "generator = Generator(noise_dim, num_features)\n",
    "discriminator = Discriminator(num_features)\n",
    "\n",
    "# LOSS FUNCTION AND OPTIMIZERS\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(X_train,y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.apply(weights_init)\n",
    "discriminator = discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x20 and 17000x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, (batch_size, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     16\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, latent_dim)))\n\u001b[1;32m---> 17\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m output_fake \u001b[38;5;241m=\u001b[39m discriminator(generated_data\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m     19\u001b[0m loss_fake \u001b[38;5;241m=\u001b[39m criterion(output_fake, fake_labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x20 and 17000x256)"
     ]
    }
   ],
   "source": [
    "model_save_freq = 100\n",
    "\n",
    "latent_dim =20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        real_data_batch = batch['input']\n",
    "        # Train discriminator on real data\n",
    "        real_labels = batch['label']\n",
    "        disc_optimizer.zero_grad()\n",
    "        output_real = discriminator(real_data_batch).reshape(256)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train discriminator on generated data\n",
    "        fake_labels = torch.FloatTensor(np.random.uniform(0, 0.1, (batch_size, 1)))\n",
    "        noise = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        generated_data = generator(noise)\n",
    "        output_fake = discriminator(generated_data.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Train generator \n",
    "        valid_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        gen_optimizer.zero_grad()\n",
    "        output_g = discriminator(generated_data)\n",
    "        loss_g = criterion(output_g, valid_labels)\n",
    "        loss_g.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}, D Loss Real: {loss_real.item()}, D Loss Fake: {loss_fake.item()}, G Loss: {loss_g.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
