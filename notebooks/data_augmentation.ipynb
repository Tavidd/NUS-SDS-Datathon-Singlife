{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./../src/\")\n",
    "from utilities import REPO_PATH, DATA_PATH, RESPONSE_COL_NAME, get_feature_corr_with_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "test = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "val = pd.read_csv(f\"{DATA_PATH}/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop([RESPONSE_COL_NAME], axis=1), train[RESPONSE_COL_NAME]\n",
    "y_test, X_test = test[RESPONSE_COL_NAME], test.drop([RESPONSE_COL_NAME],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeCounter = Counter(y_train)\n",
    "print(\"Before:\", beforeCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ratios = np.arange(0.1,1.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores_by_ratio = []\n",
    "for ratio in sample_ratios:\n",
    "    smt = SMOTE(sampling_strategy=ratio)\n",
    "    X_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n",
    "    afterCounter = Counter(y_train_sm)\n",
    "    print(\"After:\", afterCounter)\n",
    "\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(X_train_sm, y_train_sm)\n",
    "    y_preds = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_preds, y_test)\n",
    "    accuracy_scores_by_ratio.append(accuracy)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_ratios, accuracy_scores_by_ratio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a GAN to generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X.values.astype(np.float32)\n",
    "        self.labels = y.values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = {\n",
    "            'input': torch.tensor(self.data[idx]),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Block Function\n",
    "def FC_Layer_blockGen(input_dim, output_dim):\n",
    "    single_block = nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return single_block\n",
    "\n",
    "# Discriminattor Block Function   \n",
    "def FC_Layer_BlockDisc(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training parameters\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "lr = 0.0002\n",
    "num_features = 62\n",
    "latent_dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data dimensions\n",
    "noise_dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL INITIALIZATION\n",
    "generator = Generator(noise_dim, num_features)\n",
    "discriminator = Discriminator(num_features)\n",
    "\n",
    "# LOSS FUNCTION AND OPTIMIZERS\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(X_train,y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.apply(weights_init)\n",
    "discriminator = discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, D Loss Real: 41.20402908325195, D Loss Fake: 0.7985919117927551, G Loss: 0.6787103414535522\n",
      "Epoch 1, D Loss Real: 39.453125, D Loss Fake: 0.7337930202484131, G Loss: 0.6753849983215332\n",
      "Epoch 2, D Loss Real: 36.109901428222656, D Loss Fake: 0.4838208258152008, G Loss: 1.146087646484375\n",
      "Epoch 3, D Loss Real: 42.1875, D Loss Fake: 0.3636218011379242, G Loss: 1.5590752363204956\n",
      "Epoch 4, D Loss Real: 25.390625, D Loss Fake: 0.39902687072753906, G Loss: 1.7323626279830933\n",
      "Epoch 5, D Loss Real: 28.125, D Loss Fake: 0.2967832088470459, G Loss: 2.0962817668914795\n",
      "Epoch 6, D Loss Real: 28.125, D Loss Fake: 0.23845021426677704, G Loss: 2.6493773460388184\n",
      "Epoch 7, D Loss Real: 21.559921264648438, D Loss Fake: 0.2420194149017334, G Loss: 2.662781238555908\n",
      "Epoch 8, D Loss Real: 12.890625, D Loss Fake: 0.2813032269477844, G Loss: 2.8898332118988037\n",
      "Epoch 9, D Loss Real: 14.84375, D Loss Fake: 0.23095735907554626, G Loss: 3.3222343921661377\n",
      "Epoch 10, D Loss Real: 19.140625, D Loss Fake: 0.2310979813337326, G Loss: 3.3262240886688232\n",
      "Epoch 11, D Loss Real: 2.734375, D Loss Fake: 0.2268441766500473, G Loss: 3.107494354248047\n",
      "Epoch 12, D Loss Real: 2.34375, D Loss Fake: 0.2656228840351105, G Loss: 3.077035903930664\n",
      "Epoch 13, D Loss Real: 8.984375, D Loss Fake: 0.23883777856826782, G Loss: 3.1065828800201416\n",
      "Epoch 14, D Loss Real: 0.78125, D Loss Fake: 0.27389684319496155, G Loss: 2.4802792072296143\n",
      "Epoch 15, D Loss Real: 6.640625, D Loss Fake: 0.3234947621822357, G Loss: 3.5073747634887695\n",
      "Epoch 16, D Loss Real: 2.734375, D Loss Fake: 0.28470221161842346, G Loss: 3.1916351318359375\n",
      "Epoch 17, D Loss Real: 3.515625, D Loss Fake: 0.26499560475349426, G Loss: 3.2144367694854736\n",
      "Epoch 18, D Loss Real: 4.296875, D Loss Fake: 0.23533940315246582, G Loss: 3.050671100616455\n",
      "Epoch 19, D Loss Real: 2.734375, D Loss Fake: 0.21417047083377838, G Loss: 2.9142863750457764\n",
      "Epoch 20, D Loss Real: 3.90625, D Loss Fake: 0.20971645414829254, G Loss: 3.069196939468384\n",
      "Epoch 21, D Loss Real: 3.515625, D Loss Fake: 0.23651453852653503, G Loss: 2.9825868606567383\n",
      "Epoch 22, D Loss Real: 1.953125, D Loss Fake: 0.2556929886341095, G Loss: 2.9658613204956055\n",
      "Epoch 23, D Loss Real: 3.125, D Loss Fake: 0.21539126336574554, G Loss: 3.0913915634155273\n",
      "Epoch 24, D Loss Real: 3.125, D Loss Fake: 0.22033578157424927, G Loss: 3.1721513271331787\n",
      "Epoch 25, D Loss Real: 4.296875, D Loss Fake: 0.2176889181137085, G Loss: 3.001662254333496\n",
      "Epoch 26, D Loss Real: 2.734375, D Loss Fake: 0.19949601590633392, G Loss: 2.962897777557373\n",
      "Epoch 27, D Loss Real: 1.171875, D Loss Fake: 0.23260512948036194, G Loss: 2.483769655227661\n",
      "Epoch 28, D Loss Real: 3.90625, D Loss Fake: 0.20845462381839752, G Loss: 2.8470711708068848\n",
      "Epoch 29, D Loss Real: 3.125, D Loss Fake: 0.21732641756534576, G Loss: 2.9386322498321533\n",
      "Epoch 30, D Loss Real: 1.8725204467773438, D Loss Fake: 0.21964044868946075, G Loss: 2.9368748664855957\n",
      "Epoch 31, D Loss Real: 1.5625, D Loss Fake: 0.21370285749435425, G Loss: 2.9840495586395264\n",
      "Epoch 32, D Loss Real: 1.953125, D Loss Fake: 0.20271393656730652, G Loss: 2.9517064094543457\n",
      "Epoch 33, D Loss Real: 1.953125, D Loss Fake: 0.19953255355358124, G Loss: 3.0182507038116455\n",
      "Epoch 34, D Loss Real: 1.953125, D Loss Fake: 0.2068898230791092, G Loss: 2.905911445617676\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_save_freq = 100\n",
    "\n",
    "latent_dim =20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        real_data_batch = batch['input']\n",
    "        # Train discriminator on real data\n",
    "        real_labels = batch['label']\n",
    "        disc_optimizer.zero_grad()\n",
    "        output_real = discriminator(real_data_batch).reshape(256)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train discriminator on generated data\n",
    "        fake_labels = torch.FloatTensor(np.random.uniform(0, 0.1, (batch_size, 1)))\n",
    "        noise = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        generated_data = generator(noise)\n",
    "        output_fake = discriminator(generated_data.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Train generator \n",
    "        valid_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        gen_optimizer.zero_grad()\n",
    "        output_g = discriminator(generated_data)\n",
    "        loss_g = criterion(output_g, valid_labels)\n",
    "        loss_g.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}, D Loss Real: {loss_real.item()}, D Loss Fake: {loss_fake.item()}, G Loss: {loss_g.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
